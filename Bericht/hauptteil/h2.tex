\section{Ablauf bei der Verwendung von Pose2Sim}

\subsection{Übersicht des Verfahrens}
Pose2Sim fungiert als Schnittstelle zwischen Computer Vision und Biomechanik, um aus herkömmlichen Videoaufnahmen quantifizierbare Bewegungsdaten zu generieren. Der Workflow beginnt mit der Erfassung synchronisierter Rohvideos aus mehreren Perspektiven, welche im ersten Schritt durch das KI-Modell RTMPose analysiert werden. Als Zwischenergebnis liefert dieser Algorithmus reine 2D-Pixelkoordinaten sowie Konfidenzwerte für jede Kameraansicht, die anschließend durch Triangulierung in räumliche 3D-Trajektorien überführt werden. Den Abschluss bildet die Integration in die Simulationsumgebung OpenSim: Hier dienen die aufbereiteten 3D-Daten dazu, ein generisches muskuloskelettales Modell individuell an die Versuchsperson zu skalieren und schließlich mittels Inverser Kinematik die biomechanisch relevanten Gelenkwinkelverläufe zu berechnen.


\subsection{Systemvoraussetzungen}
Die Implementierung erfolgte in einer Python 3.11-Umgebung unter Verwendung von Pose2Sim (v0.10.39). Essenzielle Abhängigkeiten umfassen SciPy (v1.15.3) sowie die Anbindung an OpenSim. Die Installation und Verwaltung der virtuellen Umgebung wurde mittels Miniconda realisiert, um Versionskonflikte zu vermeiden.

\subsection{Installation von Pose2Sim}
Die Installation von Pose2Sim erfolgt über Miniconda. 
\begin{enumerate}
    \item Erstellen einer neuen Umgebung mit der passenden Python-Version: \\
    \texttt{conda create -n pose2sim python=3.11 -y}
    \item Aktivierung der Umgebung: \\
    \texttt{conda activate pose2sim}
    \item \textbf{Installation von OpenSim:} \\
    Vor der eigentlichen Pose2Sim-Installation muss die OpenSim-Umgebung eingerichtet werden: \\
    \texttt{conda install -c opensim-org opensim -y}
    \item \textbf{Installation von Pose2Sim:} \\
    Abschließend werden Pose2Sim und die Abhängigkeiten in den spezifizierten Versionen installiert: \\
    \texttt{pip install pose2sim==0.10.39 scipy==1.15.3}
\end{enumerate}


\subsection{Verwendung des pose2sim\_cli.py}
Zur Steuerung des Workflows wurde das Skript \texttt{pose2sim\_cli.py} eingesetzt. Es bündelt die Prozessschritte (Kalibrierung, Triangulierung, Filterung) und führt eine für OpenSim kritische Rotationsmatrix-Transformation durch, um Orientierungsfehler (z.\,B. invertierte Y-Achse) zu korrigieren. Das Skript ermöglicht sowohl die sequenzielle Ausführung der gesamten Pipeline als auch die modulare Ansteuerung einzelner Teilschritte.

\textbf{Bedienung und Ausführung:} \\
Das Skript wird direkt über die Kommandozeile gesteuert. Standardmäßig führt der Befehl \texttt{python pose2sim\_cli.py} den gesamten Workflow sequenziell aus. Alternativ ermöglicht das Tool eine modulare Steuerung über spezifische Flags (z.\,B. \texttt{-c} für Kalibrierung oder \texttt{-t} für Triangulierung), wodurch einzelne Prozessschritte isoliert berechnet oder wiederholt werden können.

\vspace{0.5cm}
\textbf{Verfügbarkeit des Codes:} \\
Der vollständige Quellcode, inklusive des \texttt{pose2sim\_cli.py}-Skripts sowie detaillierter Installationsanleitungen, ist im zugehörigen GitHub-Repository verfügbar: \\
\url{https://github.com/PhilippSG/Markerlose-Bewegungserfassung}


\subsection{Erstellen eines OpenSim-Modells mit Pose2Sim}
\subsubsection{Ordnerstruktur}
Für eine fehlerfreie Datenverarbeitung ist die Einhaltung einer spezifischen Ordnerstruktur zwingend erforderlich. Dies bildet die Grundlage dafür, dass sowohl die Standard-Funktionen von \textbf{Pose2Sim} als auch die Automatisierung durch das \textbf{CLI-Skript} korrekt funktionieren können.

Das Projektverzeichnis muss wie folgt organisiert sein:
\begin{verbatim}
Projekt_Verzeichnis/
├── calibration/
│   ├── intrinsics/
│   │   ├── cam01/
│   │   └── cam02/
│   └── extrinsics/
│       ├── cam01/
│       └── cam02/
├── videos/            (Enthält die Rohvideos der Bewegung)
├── Config.toml        (Zentrale Konfigurationsdatei)
└── pose2sim_cli.py    (Skript für Rotation und Automatisierung)
\end{verbatim}
Diese Struktur stellt sicher, dass Pose2Sim die Kalibrierungsdaten (in den Unterordnern \texttt{intrinsics} und \texttt{extrinsics}) eindeutig den Videodaten zuordnen kann. Gleichzeitig ermöglicht sie dem CLI-Tool, alle Schritte im Batch-Verfahren auszuführen.



\subsubsection{Kamera Setup}
Das Kamera-Setup bildet die Basis für die gesamte Datenerfassung. Ziel ist es, die Bewegungen der Versuchsperson aus möglichst vielen Perspektiven gleichzeitig aufzuzeichnen, um Verdeckungen (Okklusionen) zu minimieren.

Für eine erfolgreiche Aufnahme und zur Vermeidung typischer Fehlerquellen sind folgende Punkte entscheidend:
\begin{itemize}
    \item \textbf{Positionierung:} Es werden 2 Kameras verwendet, die um das Aufnahmevolumen herum platziert sind. Sie müssen so ausgerichtet sein, dass jedes Körperteil der Person zu jedem Zeitpunkt von mindestens zwei Kameras gesehen wird.
    \item \textbf{Zeitliche Synchronisation:} Dies ist die kritischste Fehlerquelle. Alle Kameras müssen exakt zum gleichen Zeitpunkt aufnehmen. Bereits minimale zeitliche Abweichungen führen bei schnellen Bewegungen zu erheblichen Fehlern in der 3D-Rekonstruktion.
    \item \textbf{Stabilität:} Die Kameras dürfen sich nach der Kalibrierung keinesfalls bewegen. Jede noch so kleine Positionsänderung macht die Kalibrierungsdaten unbrauchbar.
\end{itemize}


\subsubsection{Kalibrierung der Kameras}
Damit das System die 2D-Pixelkoordinaten aus den Videos in metrische 3D-Koordinaten umrechnen kann, ist eine präzise Kalibrierung zwingend erforderlich. Dieser Prozess nutzt das im \textit{Kamera Setup} gezeigte Schachbrettmuster, um die geometrischen Eigenschaften jeder Kamera zu bestimmen.


Die Kalibrierung unterteilt sich in zwei wesentliche Komponenten:

\begin{itemize}
    \item \textbf{Intrinsische Kalibrierung:} \\
    Hierbei werden die internen physikalischen Eigenschaften der Kamera berechnet. Die Ergebnisse werden für jede Kamera im Ordner \texttt{calibration/intrinsics} gespeichert.
    
    \item \textbf{Extrinsische Kalibrierung:} \\
    Hierbei wird die Position und Orientierung der Kameras im 3D-Raum relativ zueinander bestimmt. Pose2Sim berechnet eine Rotationsmatrix und einen Translationsvektor. Diese Daten landen im Ordner \texttt{calibration/extrinsics}.
\end{itemize}

\textbf{Durchführung:} \\
Die Berechnung erfolgt automatisiert durch Pose2Sim (bzw. das CLI-Tool mit dem Flag \texttt{-c}). Dabei analysiert der Algorithmus Bilder des Schachbrettmusters aus verschiedenen Winkeln und nutzt die bekannten Kantenlängen der Quadrate als Referenzmaßstab.

\textbf{Wichtig:} Damit der Algorithmus den korrekten metrischen Maßstab (Skalierung) berechnen kann, ist es entscheidend, die \textbf{Kantenlänge der Schachbrett-Quadrate} in der \texttt{Config.toml} einzutragen.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
     
        \includegraphics[height=5cm, keepaspectratio]{Kamera_Setup.jpeg}
        \caption{Kamera-Setup mit Schachbrett \\ (Extrinsische Kalibrierung)}
        \label{fig:setup_visuell}
    \end{subfigure}
    \hfill
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \includegraphics[height=5cm, keepaspectratio]{Cali_intrinsics.png}
        \caption{Intrinsische Kalibrierung}
        \label{fig:cali_visuell}
    \end{subfigure}
    
    \caption{Visualisierung der Kalibrierung.}
    \label{fig:kalibrierung_visuell}
\end{figure}


\subsubsection{Konfigurationsdatei}
Die Datei \texttt{Config.toml} fungiert als zentrale Steuerzentrale. Neben den physikalischen Parametern (wie der Schachbrettgröße) wurden basierend auf empirischen Tests spezifische Einstellungen gewählt, um die Stabilität der Pipeline zu optimieren.

Die wichtigsten Anpassungen, gegliedert nach den Sektionen der Konfigurationsdatei, umfassen:

\paragraph{[project]}
\begin{itemize}
    \item \texttt{multi\_person}: Dieser Parameter wurde dauerhaft auf \texttt{true} gesetzt. Dies deckt sowohl Einzel- als auch Mehrpersonenaufnahmen ab und vermeidet Konfigurationsfehler.
    \item \texttt{frame\_rate}: Kann auf \texttt{'auto'} belassen werden, da Pose2Sim die Bildwiederholrate der Videos zuverlässig aus den Metadaten ausliest.
    \item \texttt{frame\_range}: Es wird empfohlen, den Bereich manuell zu definieren oder alle Frames zu verarbeiten. Die Einstellung \texttt{'auto'} (automatische Erkennung, wann eine Person das Bild betritt) erwies sich in Tests als fehleranfällig.
\end{itemize}

\paragraph{[pose]}
\begin{itemize}
    \item \texttt{pose\_model}: Hier ist \texttt{body\_with\_feet} zwingend erforderlich, um für OpenSim notwendige Fuß-Landmarken (Fersen, Zehen) zu erhalten.
    \item \texttt{mode}: Die Einstellung \texttt{'performance'} bietet den besten Kompromiss aus Genauigkeit und Rechengeschwindigkeit (alternativ: \texttt{'balanced'}).
    \item \texttt{tracking\_mode}: Für das Verfolgen der Personen über die Frames hinweg lieferte der Modus \texttt{'sport2d'} deutlich robustere Ergebnisse als der \texttt{'deepsort'}-Algorithmus.
\end{itemize}

\paragraph{[synchronization]}
\begin{itemize}
    \item \texttt{synchronization\_gui}: Da die Videos bereits durch OBS Studio synchron aufgezeichnet wurden, wird dieser Wert auf \texttt{false} gesetzt. 
    \textit{Hinweis:} Bei manuellen, unsynchronisierten Aufnahmen müsste dieser Wert aktiviert werden, um die Videos nachträglich anhand eines visuellen Signals (z.\,B. Klatschen) anzugleichen.
\end{itemize}

\paragraph{[calibration]}
\begin{itemize}
    \item \texttt{type}: Standardmäßig auf \texttt{'calculate'} gesetzt.
    \item \texttt{method}: Es wird die Methode \texttt{'board'} (Schachbrett) verwendet. Die Alternative \texttt{'scene'} (Nutzung statischer Hintergrundpunkte) ist mathematisch komplexer und lieferte unzuverlässigere Ergebnisse.
    \item \textbf{Physikalische Parameter:} Entscheidend sind die korrekte Angabe der \texttt{checkerboard\_square\_size} (z.\,B. 0.03\,m) und der \texttt{corners\_nb} (Anzahl innerer Ecken), da diese als metrische Referenz für die 3D-Umrechnung dienen.
    \item \textit{Effizienz:} Die Berechnung der intrinsischen Parameter muss theoretisch nur einmalig erfolgen, solange die Objektiveinstellungen (Fokus/Zoom) der Kameras unverändert bleiben.
\end{itemize}

\subsubsection{Software und Plugins}
Zur Realisierung der synchronisierten Aufnahme kommt die Software \textbf{OBS Studio} in Kombination mit dem Plugin \textbf{Source Record} zum Einsatz. Dies ermöglicht die gleichzeitige Aufzeichnung beider Kameras.


\subsubsection{2D Pose Erkennung}
Nach der Datenerfassung folgt der erste Berechnungsschritt: die Extraktion anatomischer Landmarken aus den Rohvideos. Hierbei kommt das Deep-Learning-Modell \textbf{RTMPose} zum Einsatz, welches in der Konfigurationsdatei definiert wurde.

Der Prozess läuft wie folgt ab:
\begin{enumerate}
    \item \textbf{Detektion:} Der Algorithmus analysiert bildweise (frame-by-frame) die Aufnahmen aller Kameras separat. Dabei werden die Positionen relevanter Körpergelenke (z.\,B. Schulter, Ellbogen, Handgelenk) im zweidimensionalen Bildraum identifiziert.
    \item \textbf{Datenstruktur:} Für jeden Keypoint generiert das Modell einen Datensatz bestehend aus:
    \begin{itemize}
        \item Den Pixel-Koordinaten $(x, y)$.
        \item Einem Konfidenzwert (Likelihood), der angibt, mit welcher Wahrscheinlichkeit der Punkt korrekt erkannt wurde.
    \end{itemize}
    \item \textbf{Speicherung:} Die Ergebnisse werden als JSON-Dateien im Ordner \texttt{pose} abgelegt.
\end{enumerate}

Die Ausführung erfolgt automatisiert über das CLI-Tool mit dem Flag \texttt{-p}. Das Resultat ist eine Sammlung von unabhängigen 2D-Skelettmodellen aus verschiedenen Perspektiven, die noch keine räumliche Tiefe besitzen.

\begin{figure}[ht]
    \centering
    % Bild links
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        % 
        % 
        \includegraphics[width=\linewidth, height=5cm, keepaspectratio]{Handshake.png}
        \caption{2D-Pose-Erkennung (RTMPose)}
        \label{fig:handshake_2d}
    \end{subfigure}
    \hfill % 
    % Bild rechts
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=\linewidth, height=5cm, keepaspectratio]{handshake opensim.png} 
        \caption{OpenSim-Modell (Inverse Kinematik)}
        \label{fig:handshake_opensim}
    \end{subfigure}
    
    \caption{\textbf{Von 2D zu 3D:} Visualisierung einer komplexen Interaktion (Händeschütteln). Links ist die robuste Detektion der Keypoints durch RTMPose zu sehen. Rechts das daraus generierte, muskuloskelettale OpenSim-Modell, das die Bewegung räumlich korrekt wiedergibt.}
    \label{fig:handshake_comparison}
\end{figure}


\subsubsection{Triangulierung}
Der zentrale Schritt der Pipeline ist die Überführung der planaren 2D-Daten in ein räumliches 3D-Modell. Dies geschieht durch das mathematische Verfahren der \textbf{Triangulierung}.

Dabei werden die 2D-Informationen aus den verschiedenen Kameraperspektiven unter Einbeziehung der Kalibrierungsdaten (Position und Orientierung der Kameras) fusioniert. Für jeden Gelenkpunkt wird der Schnittpunkt der Sichtstrahlen im 3D-Raum berechnet.

\textbf{Qualitätssicherung:} \\
Ein entscheidender Vorteil von Pose2Sim ist die gewichtete Triangulierung. Der Algorithmus berücksichtigt den in der vorherigen Stufe ermittelten \textbf{Konfidenzwert} (Likelihood).
\begin{itemize}
    \item Punkte mit hoher Erkennungswahrscheinlichkeit gehen stärker in die Berechnung ein.
    \item Punkte unterhalb des in der \texttt{Config.toml} definierten Schwellenwerts (\texttt{likelihood\_threshold}) werden ignoriert, um Ausreißer zu minimieren.
\end{itemize}

Dieser Schritt wird über das CLI-Tool mit dem Flag \texttt{-t} ausgeführt. Das Ergebnis ist eine Punktwolke (Point Cloud) der Gelenke im metrischen Raum ($x, y, z$ in Metern).

\subsubsection{TRC Datei Erstellung}
Der letzte Schritt der Pipeline ist die Generierung der finalen Bewegungsdaten. Das primäre Zielformat ist das \textbf{.trc-Format} (Track Row Column), welches die Trajektorien aller virtuellen Marker über die Zeit speichert.

Bevor die Dateien exportiert werden, durchlaufen die Daten zwei wesentliche Nachbearbeitungsschritte:

\begin{itemize}
    \item \textbf{Filterung (Filtering):} \\
    Da die rohen triangulierten Daten oft ein hochfrequentes Rauschen (Jitter) aufweisen, wird ein \textbf{Butterworth-Tiefpassfilter} angewendet. Die in der Konfiguration definierte Grenzfrequenz (z.\,B. 6\,Hz) sorgt dafür, dass das unnatürliche Zittern der Keypoints entfernt wird, während die eigentliche menschliche Bewegung erhalten bleibt.

    \item \textbf{Rotation und Koordinatentransformation:} \\
    Dies ist ein kritischer Punkt für die Kompatibilität. Ohne Korrektur würde das Modell in OpenSim falsch orientiert erscheinen (z.\,B. \textbf{Kopf nach unten}). Das CLI-Tool führt automatisch eine Rotationsmatrix-Transformation durch, um die Daten an das Welt-Koordinatensystem von OpenSim anzupassen.
\end{itemize}

\textbf{Ergebnis und generierte Dateien:} \\
Nach Abschluss des Prozesses liegen im Ordner \texttt{pose-3d} alle für die Simulation notwendigen Dateien bereit. Neben der \texttt{.trc}-Datei generiert Pose2Sim automatisch:
\begin{itemize}
    \item \textbf{.osim (OpenSim Modell):} Eine Modelldatei, die bereits das korrekte, virtuelle Marker-Set (basierend auf \texttt{body\_with\_feet}) enthält. Dies erspart das manuelle Hinzufügen von Markern in der OpenSim-GUI.
    \item \textbf{.mot (Motion File):} Eine ergänzende Bewegungsdatei, die Referenzdaten oder Koordinaten für die weitere Verarbeitung enthält.
\end{itemize}
Damit ist der Datensatz vollständig und kann ohne weitere Konvertierung direkt in OpenSim geladen werden.

\subsection{Datenverarbeitung in OpenSim}
Nachdem die videobasierten Daten in Pose2Sim aufbereitet wurden, erfolgt die biomechanische Analyse in der Simulationsumgebung \textbf{OpenSim}. Dieser Prozessschritt transformiert die reinen Positionsdaten (Trajektorien) in physiologisch bedeutsame Gelenkwinkel.

\subsection{Modell-Import und Vorbereitung}
Ein wesentlicher Vorteil des verwendeten Workflows ist die nahtlose Integration der Daten. Anstatt ein Standard-Modell manuell mit Markern zu versehen, wird das von Pose2Sim im Ordner \texttt{pose-3d} generierte \textbf{.osim-Modell} geladen.

Dieses Modell enthält bereits ein \textbf{virtuelles Markerset}, das exakt den anatomischen Landmarken des verwendeten Pose-Modells (\texttt{body\_with\_feet}) entspricht. Parallel dazu wird die erstellte \texttt{.trc}-Datei als Bewegungsdatenquelle importiert.

\subsection{Modell-Erstellung und Skalierung}
Ein besonderer Vorteil der verwendeten Pipeline ist die automatisierte Erstellung eines personen-spezifischen Modells. Im klassischen Workflow müsste ein Standard-Modell in OpenSim manuell skaliert werden.

Hier erfolgt der Prozess datengesteuert durch Pose2Sim:
\begin{enumerate}
    \item \textbf{Segment-Anpassung:} Der Algorithmus berechnet aus den triangulierten 3D-Koordinaten die tatsächlichen Segmentlängen der Versuchsperson (z.\,B. Distanz zwischen Hüft- und Kniegelenk).
    \item \textbf{Modell-Generierung:} Basierend auf diesen Maßen wird das generische OpenSim-Modell geometrisch angepasst (skaliert).
    \item \textbf{Marker-Platzierung:} Gleichzeitig werden die virtuellen Marker exakt an den Positionen der erkannten Gelenke platziert.
\end{enumerate}

Das Resultat ist die im Ergebnisordner vorliegende \texttt{.osim}-Datei. Dieses Modell repräsentiert bereits die Statur der Versuchsperson und kann direkt für die weitere Analyse verwendet werden, ohne dass eine manuelle Skalierung notwendig ist.

\subsection{Inverse Kinematik (IK)}
Der finale Schritt der Datenverarbeitung ist die Berechnung der tatsächlichen Gelenkwinkel. Hierbei kommt das Verfahren der \textbf{Inversen Kinematik (IK)} zum Einsatz.

\textbf{Funktionsweise:} \\
Bei der Inversen Kinematik wird das (automatisch skalierte) Modell durch die Bewegungsdaten der \texttt{.trc}-Datei „angetrieben“. Für jeden einzelnen Zeitschritt (Frame) löst OpenSim ein mathematisches Optimierungsproblem.
Das Ziel ist es, die Gelenkwinkel des Modells so einzustellen, dass der Abstand zwischen den \textit{experimentellen Markern} (aus der Pose2Sim-Berechnung) und den \textit{virtuellen Markern} auf dem Modell minimiert wird.

\textbf{Ergebnis (Endpunkt):} \\
Das Resultat dieses Prozesses ist eine \textbf{.mot-Datei} (Motion File). Diese Datei enthält die finalen, zeitabhängigen \textbf{Gelenkwinkel} (in Grad) für alle Freiheitsgrade des Modells.

Damit ist der gesamte Workflow abgeschlossen: Aus den ursprünglichen 2D-Rohvideos wurden quantifizierbare, biomechanische Kinematikdaten gewonnen, die nun für die weitere grafische Auswertung oder statistische Analyse bereitstehen.

\subsection{Visualisierung der Ergebnisse in OpenSim}
Nach der Berechnung dient die grafische Benutzeroberfläche (GUI) von OpenSim zur visuellen Validierung der Bewegungsdaten.

\paragraph{Visualisierung eines einzelnen Modells}
Um die berechnete Kinematik einer einzelnen Aufnahme zu betrachten:
\begin{enumerate}
    \item \textbf{Modell laden:} Über \textit{File $\rightarrow$ Open Model} wird die generierte \texttt{.osim}-Datei geöffnet.
    \item \textbf{Bewegungsdaten laden:} Über \textit{File $\rightarrow$ Load Motion} wird die zugehörige \texttt{.mot}-Datei (aus der IK-Berechnung) importiert.
    \item \textbf{Wiedergabe:} Die Bewegung kann nun über die Zeitleiste (Seek Bar) abgespielt oder bildweise analysiert werden.
\end{enumerate}

\paragraph{Synchronisieren mehrerer Modelle (Vergleich)}
Um beispielsweise zwei verschiedene Versuche oder Perspektiven gleichzeitig abzuspielen (z.\,B. für einen Vorher-Nachher-Vergleich):
\begin{enumerate}
    \item Beide Modelle und deren zugehörige Motion-Dateien (.mot) in OpenSim laden.
    \item Im \textit{Navigator}-Fenster (links) die Option \textit{Coordinates} beider Modelle gleichzeitig markieren (mit \texttt{Strg} + Mausklick).
    \item Rechtsklick auf die markierten Elemente und die Option \textit{Sync. Motions} wählen.
    \item Beim Abspielen laufen nun beide Modelle exakt synchron.
\end{enumerate}

\subsection{Ergebnisse von Pose2Sim}
Die durchgeführte Pipeline ermöglichte die erfolgreiche Extraktion kinematischer 3D-Daten aus den synchronisierten Videoaufnahmen. Bevor die physiologischen Gelenkwinkel betrachtet werden, erfolgt zunächst eine Analyse der Signalqualität.

\subsubsection{Qualität der Datenaufbereitung und Filterung}
Ein wesentlicher Schritt der Datenverarbeitung war die Reduktion des hochfrequenten Rauschens (Jitter), das bei markerlosen Tracking-Verfahren systembedingt auftritt.

Abbildung 2.6 demonstriert die Effektivität des angewendeten Butterworth-Tiefpassfilters am Beispiel der 3D-Koordinaten des Hüftgelenks (Hip):

\begin{itemize}
    \item \textbf{Ungefiltertes Signal (Blau):} Die rohen Triangulierungsdaten zeigen deutliche, hochfrequente Ausschläge. Diese Instabilität würde in der Inversen Kinematik zu fehlerhaften Gelenkwinkelberechnungen führen.
    \item \textbf{Gefiltertes Signal (Orange):} Der Filter (Cut-off: 6\,Hz) glättet den Signalverlauf effektiv. Entscheidend ist hierbei, dass die grundlegende Form der Bewegungskurve erhalten bleibt (kein „Over-Smoothing“), während die Störsignale eliminiert werden.
\end{itemize}

\begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{person00_Hip.png} 
    \caption{\textbf{Einfluss der Filterung auf die 3D-Trajektorien:} Darstellung der X-, Y- und Z-Koordinaten des Hüftgelenks über die Zeit. Der Vergleich zwischen rohen (blau) und gefilterten (orange) Daten belegt die notwendige Stabilisierung des Signals für die weitere Analyse in OpenSim.}
    \label{fig:filter_check}
\end{figure}

Die hier gezeigte Signalglättung bildet die notwendige Basis für die darauf folgende Berechnung der Gelenkwinkel.

\subsubsection{Generierung der Simulationsdaten (.mot)}
Aufbauend auf den bereinigten Trajektorien konnten im nächsten Schritt die Motion-Files (\texttt{.mot}) generiert werden. Diese Dateien stellen das finale Ergebnis der kinematischen Berechnung dar und beinhalten die zeitlichen Verläufe aller Gelenkwinkel (in Grad) für die Freiheitsgrade des Modells.

Eine Überprüfung der Daten zeigt physiologisch plausible Wertebereiche (z.\,B. für Knie- und Hüftflexion), die frei von den in den Rohdaten beobachteten hochfrequenten Störungen sind. Damit stehen validierte Datensätze bereit, die direkt in der OpenSim-GUI visualisiert oder für weiterführende biomechanische Analysen herangezogen werden können.


\subsubsection{Kritische Einflussfaktoren}
Die Evaluation der Pipeline identifizierte folgende Parameter als qualitätsbestimmend:

\begin{itemize}
    \item \textbf{Hardwareseitige Restriktionen (Kamera-Anzahl):} 
    Der Versuchsaufbau beschränkte sich auf ein Dual-Kamera-Setup. Voruntersuchungen zeigten, dass die Einbindung weiterer Perspektiven auf der verwendeten mobilen Workstation (Intel Core i5-11400H, 16GB RAM, NVIDIA RTX 3060) zu einer instabilen Bildrate (Frame Drops) führte. 
    Dieser Leistungsengpass ist primär auf die begrenzte \textbf{Video-Encodierungs-Bandbreite} der GPU zurückzuführen, welche eine simultane Echtzeitverarbeitung von mehr als zwei HD-Streams verhinderte.

    \item \textbf{Beleuchtung \& Kontrast:} 
    Eine ausreichende Ausleuchtung ist zwingend. Dunkle Kleidung in dunkler Umgebung führte zu massivem Signalverlust („Teleportation“).

    \item \textbf{Kleidung:} 
    Eng anliegende Kleidung (Tight-Fit) ist essenziell, da weite Stoffe die Lokalisierung der anatomischen Gelenkzentren verfälschen.

    \item \textbf{Okklusion:} 
    Verdeckungen durch externe Objekte (z.\,B. Rollstuhl) oder Selbstverdeckung (z.\,B. verschränkte Arme) führen zu Tracking-Aussetzern und Artefakten.

    \item \textbf{Foot Sliding:} 
    Da keine Bodenreaktionskräfte gemessen werden, kann es im Modell zu minimalem Rutschen der Füße kommen.
\end{itemize}

\subsubsection{Systemgrenzen und Beobachtungen}
\textbf{Stärken (Positiv):}
\begin{itemize}
    \item \textbf{Komplexe Interaktionen:} 
    Das System erfasst nicht nur zyklisches Gehen, sondern auch komplexe, nicht-zyklische Bewegungen wie \textbf{Händeschütteln} oder Bewegungen in \textbf{sitzender Position} (im Stuhl).
    
    \item \textbf{Robuste Identifizierung (ID-Tracking):} 
    Eine bemerkenswerte Stärke ist die Trennschärfe bei mehreren Personen. Der Algorithmus unterscheidet Individuen zuverlässig, selbst wenn diese eine \textbf{ähnliche Statur, identische Kleidung} und gleiche Bewegungsmuster aufweisen.
    
    \item \textbf{Anthropometrische Skalierung:} 
    Die generierten OpenSim-Modelle bilden individuelle \textbf{Körpergrößen und Proportionen} der Teilnehmer maßstabsgetreu ab, sodass Größenunterschiede in der Simulation visuell und rechnerisch exakt wiedergegeben werden.
    
    \item \textbf{Tiefenstabilität:} 
    Das System unterscheidet robust zwischen aktiven Personen im Vordergrund und störenden Bewegungen im Hintergrund.
\end{itemize}

\textbf{Schwächen (Limitierungen):}
\begin{itemize}
    \item \textbf{Beleuchtung:} Kombinationen aus dunkler Kleidung und schwachem Licht führen zu massivem Signalverlust („Teleportation“).
    \begin{figure}[H]
    \centering
    \includegraphics[width=1.0\textwidth]{Error.png} 
    \caption{\textbf{Tracking-Abbruch bei Dunkelheit:} Auszug aus dem System-Log. Aufgrund mangelnden Kontrasts, schlechter Kalibrierung sowie unvollständiger Abbildung der Person im Frame konnte keine kontinuierliche Erkennung gewährleistet werden.}
    \label{fig:error_dark}
\end{figure}
    \item \textbf{Okklusion (Verdeckung):} 
    Neben statischen Objekten (z.\,B. Rollstuhl) stellen insbesondere *dynamische Überlappungen* eine Fehlerquelle dar. Bei Multi-Personen-Aufnahmen ist eine strikte räumliche Trennung der Akteure zwingend erforderlich. Kreuzen sich die Laufwege der Personen (Crossing), kann der Algorithmus die Gelenke nicht mehr eindeutig zuordnen (Identity Switch), was zum Zusammenbruch der Simulation führt.

\begin{figure}[H]
    \centering
    % --- Bild 1: Vorher ---
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{8 walk vor überschneidung.png}
        \caption{Vor der Kreuzung: Korrekte IDs (0 und 1).}
        \label{fig:cross_before}
    \end{subfigure}
    \hfill
    % --- Bild 2: Während ---
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{8 walk bei der Überschneidung.png}
        \caption{Überlappung: Verlust der Skelettstruktur.}
        \label{fig:cross_during}
    \end{subfigure}
    \hfill
    % --- Bild 3: Nachher ---
    \begin{subfigure}[b]{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{8 walk nach überschneidung.png}
        \caption{Nach der Kreuzung: Identity Switch (ID 0 $\to$ 4).}
        \label{fig:cross_after}
    \end{subfigure}
    
    \caption{\textbf{Fehleranalyse bei dynamischer Okklusion (Crossing):} Die Bildsequenz demonstriert das Versagen des Trackings bei sich kreuzenden Laufwegen. 
    (a) Die Personen werden korrekt getrackt (Blau: ID 0, Rot: ID 1). 
    (b) Während der Überlappung kann der Algorithmus die Gliedmaßen nicht mehr zuordnen. 
    (c) Nach der Trennung erhält die männliche Person fälschlicherweise eine neue ID (Gelb: ID 4 statt Blau: ID 0), was die Kontinuität der Simulation unterbricht.}
    \label{fig:occlusion_sequence}
\end{figure}
    
    \item \textbf{Foot Sliding:} Da keine Kraftmessplatten verwendet werden, neigen die Füße im Modell zu leichtem Rutschen auf dem Boden.
    \item \textbf{Bewegungsunschärfe:} Sehr schnelle Bewegungen führen zu ungenauen Keypoints und erfordern stärkere Filterung.
\end{itemize}





