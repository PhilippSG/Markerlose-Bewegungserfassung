\chapter{Hauptteil}
\section{Ablauf bei der Verwendung von MediaPipe}
Im folgenden wird unserer Workflow beschrieben, den wir verwendet haben um mit MediaPipe zum Ziel zu kommen. 
\subsection{Importierung der Bibliotheken}
Es werden folgende Versionen der Module für die Verwendung mit Python verwendet:
\begin{itemize}
    \item csv (kommt mit python) 3.12.12 \newline
    Standard-Modul zum Schreiben von Textdateien im "Comma Separated Values" - Format.
    \item math (kommt mit python) 3.12.12 \newline
    Standard-Mathematik-Bibliothek von Python.
    \item cv2 4.12.0.88 (opencv-python) \newline
    Bibliothek für Computer Vision. Ist Notwendig zum lesen der Video Dateien und weiteren Bearbeitung. 
    \item MediaPipe 0.10.21 \newline
    Machine-Learning-Bibliotheken von Google. Wichtig für die Pose Schätzung.
    \item pandas 2.3.3 \newline
    Modul für die Datenanalyse und Tabellenkalkulation. Wichtig für die Umwandlung von CSV in TRC Datei.
    \item numpy 1.26.4 \newline
    Standardbibliothek für mathematische Berechnungen mit Vektoren und Matrizen. Wichtig für die Skalierung.
\end{itemize}
\subsection{Anzeigen der Videos}
Wir haben eine Funktion erstellt mit der unsere Videos, mit den eingezeichneten Skeletten von MediaPipe, angezeigt werden.
\begin{lstlisting}[language=Python, caption= Funktion zum Abspielen der Videos]
def play_video(video_path, rotate_video_by_degrees, mp_pose, display_marker_names=False):
    """Displays the video with markers from MediaPipe, but without their names."""
    cap = cv2.VideoCapture(video_path)

    pose = create_pose(mp_pose)
    
    mp_drawing = mp.solutions.drawing_utils

    while cap.isOpened():
        success, frame = cap.read()
            
        if not success:
            break

        if (rotate_video_by_degrees != 0):
            if (rotate_video_by_degrees == 90):
                frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
            elif(rotate_video_by_degrees == 180):
                frame = cv2.rotate(frame, cv2.ROTATE_180)
            elif(rotate_video_by_degrees == 270):
                frame = cv2.rotate(frame, cv2.ROTATE_270)     
            
        # BGR -> RGB
        image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        results = pose.process(image_rgb)

        if results.pose_landmarks:
            mp_drawing.draw_landmarks(
                frame,
                results.pose_landmarks,
                mp_pose.POSE_CONNECTIONS,
                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),
                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2)
            )

        if (display_marker_names):
            h, w, _ = frame.shape
            for idx, landmark in enumerate(results.pose_landmarks.landmark):
                cx, cy = int(landmark.x * w), int(landmark.y * h)
                name = mp_pose.PoseLandmark(idx).name
                cv2.putText(frame, name, (cx + 5, cy - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 255), 1, cv2.LINE_AA)

        cv2.imshow("Video", frame)

        if cv2.waitKey(5) & 0xFF == ord('q'):
            break
    
    cap.release()
    cv2.destroyAllWindows()
\end{lstlisting}
Die Funktion benötigt vier Übergabevariablen um das Video anzuzeigen:
\begin{enumerate}
    \item \verb|videopath|: Der Pfad zum Video
    \item \verb|rotate_video_by_degrees|: Die Gradzahl, um die das Video beim Anzeigen rotiert wird.
    \item \verb|mp_pose|: Die von MediaPipe erstellte Posen-Schätzung
    \item \verb|display_marker_names|: Flag Variable, um das Video mit benannten Markern zu erstellen. Standardmäßig aus.
\end{enumerate}
\begin{figure}[ht]
    \centering
    % Erstes Unter-Bild
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{videoframe.png}
        \caption{Ohne Marker-Namen}
        \label{fig:frame_no_names}
    \end{subfigure}
    \hfill % Fügt flexiblen Platz zwischen den Bildern ein
    % Zweites Unter-Bild
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{videoframe_names.png}
        \caption{Mit Marker-Namen}
        \label{fig:frame_with_names}
    \end{subfigure}
    
    % Gemeinsame Haupt-Beschriftung
    \caption{Vergleich der Video-Ausgabe aus dem MediaPipe Workflow}
    \label{fig:video_comparison}
\end{figure}
\newpage
\subsection{Erstellen einer CSV mit MediaPipe Daten aus einem brauchbaren Video}
Als nächstes haben wir die Daten aus MediaPipe in eine CSV geschrieben um mit dieser dann weiterzuarbeiten.
\begin{lstlisting}[language=Python,caption={Funktion welche die MediaPipe Daten in eine CSV Datei schreibt}]
def pose_estimation(video, landmark_map, video_folder, output_folder, rotate_video_by_degrees, flip_x_axis, mp_pose):
    """Runs the MediaPipe pose estimation on the video and extracts marker positions into a csv file."""
    print("Running pose estimation...")
    csv_path = output_folder + video + ".csv"
    video_path = video_folder + video + ".mp4"
    
    cap = cv2.VideoCapture(video_path)
    
    pose = create_pose(mp_pose)
    
    # CSV-File
    with open(csv_path, mode='w', newline='') as file:
        writer = csv.writer(file)
    
        # Meta/FPS
        writer.writerow(["#FPS", cap.get(cv2.CAP_PROP_FPS)])
    
        # Header: Frame + Marker X/Y/Z
        header = ["Frame"]
        
        for name, mp_name in landmark_map.items():
            header += [f"{name}_X", f"{name}_Y", f"{name}_Z"]
        writer.writerow(header)
    
        frame_idx = 0
    
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break
                
            if (rotate_video_by_degrees != 0):
                if (rotate_video_by_degrees == 90):
                    frame = cv2.rotate(frame, cv2.ROTATE_90_CLOCKWISE)
                elif(rotate_video_by_degrees == 180):
                    frame = cv2.rotate(frame, cv2.ROTATE_180)
                elif(rotate_video_by_degrees == 270):
                    frame = cv2.rotate(frame, cv2.ROTATE_270)
                    
            # BGR -> RGB
            image_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            results = pose.process(image_rgb)
    
            row = [frame_idx]
    
            if results.pose_landmarks:
                landmarks = results.pose_landmarks.landmark
    
                for name, mp_name in landmark_map.items():    
                    lm = landmarks[mp_name]
                    
                    if (flip_x_axis):
                        lm.x = -lm.x
                    # invert y axis
                    row += [lm.x, -lm.y, lm.z]
    
            else:
                # if no landmark - fill with 0s
                row += [0.0] * (len(landmark_map.items()) * 3)
    
            writer.writerow(row)
            frame_idx += 1
    cap.release()
    pose.close()
    print("Pose estimation complete and output file successfully created: " + csv_path)
\end{lstlisting}
Als Ausgabe erhält man dann eine CSV Datei mit folgenden Informationen welche für die Tracer Datei später wichtig sind.
Die generierte CSV-Datei folgt einer strikten Struktur, die dynamisch anhand der definierten Marker-Liste erstellt wird. 
\begin{itemize}
    \item \textbf{Zeile 1:} Metadaten zur Framerate (FPS).
    \item \textbf{Zeile 2 (Header):} Spaltenbeschriftungen. Für jeden Marker $n$ werden automatisch drei Spalten (\texttt{Marker$n$\_X}, \texttt{Marker$n$\_Y}, \texttt{Marker$n$\_Z}) generiert.
    \item \textbf{Zeile 3 ff.:} Die Positionsdaten pro Frame.
\end{itemize}

\begin{lstlisting}[caption={Allgemeine Struktur der Ausgabedatei}, label={lst:csv_generic}]
#FPS,30.0
Frame,Marker1_X,Marker1_Y,Marker1_Z,Marker2_X,Marker2_Y,Marker2_Z
0,0.12,-0.55,0.80,0.45,-0.30,0.12
1,0.13,-0.54,0.81,0.46,-0.31,0.13
2,0.13,-0.54,0.81,0.46,-0.31,0.13
...
\end{lstlisting}
\newpage
\subsection{Skalierung der MediaPipe Daten}
Da MediaPipe die erkannten 3D-Koordinaten in einem normierten Raum zurückgibt, müssen diese Koordinaten
skaliert werden, um sie in OpenSim verwenden zu können. Dabei geht es hauptsächlich darum, die Längen
von Körperteilen korrekt darzustellen und nicht über die frames variieren zu lassen und den Ursprung der
MediaPipe Daten auf den Ursprung von OpenSim zu verschieben.

Für die Ursprungsverschiebung muss ein Marker als Referenz gewählt und dessen Position in OpenSim als
Referenzkoordinate übergeben werden. Dann wird die durchschnittliche Koordinate dieses Markers über die
statische Aufnahme berechnet und der Differenzvektor zu der OpenSim Referenzkoordinate bestimmt. Dieser
Vektor wird dann auf alle MediaPipe Koordinaten addiert, um den Ursprung zu verschieben.

Die Skalierung der Körperteile kann auf verschiedene Arten erfolgen:
\begin{itemize}
    \item \textbf{Skalierung über die durchschnittliche Länge in der statischen Aufnahme:} Hier werden die Körperteile
    einfach auf die durchschnittlichen Längen in der statischen Aufnahme skaliert.
    \item \textbf{Skalierung über eine bekannte Referenzlänge:} Hierbei wird eine bekannte Länge 
    (z.B. die Länge eines Armes) verwendet. Dann wird ein Faktor zwischen dieser und der durchschnittlichen Länge des verwendeten
    Körperteils in der statischen Aufnahme berechnet und anschließend auf alle durchschnittlichen Längen aller Körperteile angewendet,
    um die skalierten Längen zu erhalten.

    Dies ist besonders nützlich, wenn die Person in der Aufnahme eine erhöhte Distanz zur Kamera hat.
\end{itemize}

Um die eigentliche Skalierung durchführen zu können, müssen zuerst die zu skalierenden Körperteile definiert werden.
Für eine saubere Skalierung und um Winkel zwischen den Körperteilen beizubehalten, können die Körperteile in Gruppen
zusammengefasst werden. Jede Gruppe kann aus mehreren verbundenen Körperteilen bestehen, die zusammen skaliert werden.
Im Code gibt es dafür das Array \texttt{limb\_groups}, welches die Indizes der
zu skalierenden Körperteile im \texttt{landmark\_map} dictionary als Tupel übergibt.
Bei einem einfachen Arme Modell, bestehend aus 6 Markern (jeweils Schulter, Ellbogen, Handgelenk), könnte dies wie folgt aussehen:
\begin{verbatim}
idx Marker_name
0   r_shoulder
1   r_elbow
2   r_wrist
3   l_shoulder
4   l_elbow
5   l_wrist


limb_groups = [
    [(0, 1), (1, 2)],
    [(3, 4), (4, 5)]
]
\end{verbatim}

Bei der Skalierung wird für jedes Tupel der zweite Marker an den ersten angepasst, daher ist die Reihenfolge hier entscheidend.
Nach der Verschiebung eines Markers wird ein Verschiebungsvektor aufaddiert, welcher vor der Skalierung des nächsten
Körperteils auch auf den zweiten Marker des Tupels angewendet wird, um die Winkel beizubehalten.

Wenn der entsprechende Parameter in der \texttt{scale} Funktion gesetzt ist, werden die
zu skalierenden Längen einmal vor (Fig. \ref{fig:unscaled_plot}) und nach (Fig. \ref{fig:scaled_plot}) der Skalierung in einem Diagramm dargestellt, um die Skalierung zu überprüfen.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/lengths_before_scaling_s.png}
        \caption{Längen vor Skalierung}
        \label{fig:unscaled_plot}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/lengths_after_scaling_s.png}
        \caption{Längen nach Skalierung}
        \label{fig:scaled_plot}
    \end{subfigure}

    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/scaling_legend.png}
        \label{fig:scaled_legend}
    \end{subfigure}

    \caption{Skalierungs Diagramme}
\end{figure}

Sind nach der Skalierung noch Abweichungen in den Längen der Körperteile vorhanden, so liegt wahrscheinlich ein
Fehler in der Definition der \texttt{limb\_groups} vor.

\subsection{Erstellen einer Tracer(.trc) Datei aus den Daten in der CSV Datei}
\subsection{Verwendung der Tracer Datei um die virtuellen Marker als experimentelle Marker in OpenSim anzeigen zu lassen}
\subsection{Anwendung auf ein Modell in OpenSim}
