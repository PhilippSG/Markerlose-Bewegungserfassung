
\section{Vergleich zweier Ansätze zur markerlosen Pose-Erfassung}
In diesem Kapitel werden zwei Ansätze zur markerlosen Pose-Erfassung
untersucht und miteinander verglichen: zum einen ein eigener Workflow mit MediaPipe als Ansatz zur
2D- und 3D- Poseschätzung sowie zum anderen ein fertiger Open Source Workflow für OpenSim
\cite{pose2sim} (mit Verwendung neuerer Modelle wie RTM Pose) zur Rekonstruktion biomechanischer Modelle.
Die Motivation für diesen Vergleich liegt darin begründet, dass beide Varianten im Kern 
denselben algorithmischen Berechnungsschritten folgen, sich jedoch in ihrer systemseitigen Umsetzung unterscheiden. 
Während Pose2Sim als integrierte Lösung fungiert, in der die gesamte Prozesskette bereits implementiert ist, 
führt der in 2.4 betrachtete MediaPipe-Workflow die Einzelschritte sequenziell aus 
und beschränkt sich dabei auf die Nutzung eines monokularen Kameraystems.

Des Weiteren erfolgt ein Genauigkeitsvergleich, bei dem die Pose2Sim-Pipeline als Referenz dient, 
um die Eignung des MediaPipe-Ansatzes für biomechanische Anwendungen zu bewerten. 
Ergänzend wird die Robustheit der Pose2Sim-Pipeline gegenüber Störfaktoren wie Kleidung 
oder unterschiedlichen Beleuchtungsbedingungen analysiert.
\newpage
\section{Ablauf bei der Verwendung von MediaPipe}
Zu Beginn brauchen wir aus unseren 2D Daten, in Form eines Videos von unserer Kamera,
geschätzte 3D-Koordinaten von markanten Punkten am Körper (Keypoints).
Wir haben dabei das BlazePose Modell verwendet welches 33 Keypoints liefert.
\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{MediaPipe-pose-BlazePose-Topology.jpg}
    \label{fig:BlazePose Topologie} 
\end{figure}

Fokussiert haben wir uns dabei erstmal auf drei Keypoints um eine bessere Übersicht zu behalten.
\begin{itemize}
    \item 11. rechte Schulter
    \item 13. rechter Ellbogen
    \item 15. rechtes Handgelenk
\end{itemize}
\newpage
Unser Ziel ist es, diese drei Keypoints aus MediaPipe BlazePose auf ein Modell in OpenSim zu übersetzen
und ,mithilfe des Inversen Kinematik Tools von OpenSim \cite{opensim_ik}, Gelenkwinkel von einer Bewegung herauszubekommen.
Diese können wir dann mit den Gelenkwinkeln vom Pose2Sim Ansatz vergleichen.
Dafür haben wir zwei unterschiedliche OpenSim Modelle  verwendet:

\begin{itemize}
    \item \texttt{arm\_model}\cite{opensim_arm_model}-- Ein auf dem Standard-Modell arm26\cite{opensim_models} von OpenSim aufbauendes Modell, welches sich nur auf den rechten Arm konzentriert.
    \item \texttt{Model\_Pose2Sim\_muscles\_flex}\cite{pose2sim_setup} -- Ein Ganzkörper Modell von pose2sim für den besseren Vergleich.
\end{itemize}
\begin{figure}[ht]
    \centering
    % Erstes Unter-Bild
    \begin{subfigure}[bx]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{arm26_snapshot.png}
        \caption{arm\_model}
        \label{fig:arm26}
    \end{subfigure}
    \hfill % Fügt flexiblen Platz zwischen den Bildern ein
    % Zweites Unter-Bild
    \begin{subfigure}[bx]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{Model_Pose2Sim_muscles_flex_snapshot.png}
        \caption{Model\_Pose2Sim\_muscles\_flex}
        \label{fig:Model_Pose2Sim_muscles_flex}
    \end{subfigure}
    
    % Gemeinsame Haupt-Beschriftung
    \caption{Vergleich der verwendeten OpenSim Modelle}
    \label{fig:model_comparison}
\end{figure}
Für dieses Ziel sind vier wesentliche Schritte notwendig:
\begin{enumerate}
    \item Erhalten der Daten aus einem Video mithilfe von MediaPipe BlazePose
    \item Skalierung 
    \item Erstellen einer Marker Datei für OpenSim
    \item Inverse Kinematik
\end{enumerate}
Das Ergebnis des Inversen Kinematik Tools von OpenSim ist dann eine Bewegungs Datei (.mot).
In dieser finden wir dann die Gelenkwinkel zu bestimmten Zeitpunkten.

Nun folgt eine Beschreibung des Workflows. \newpage
\subsection{Softwareversionen und Module}
Wir haben uns für die Programmiersprache Python entschieden, da es eine einfache und schnelle Möglichkeit bietet,
um Bildverarbeitungs Bibliotheken zu verwenden. Außerdem ist Python sehr verbreitet in der
Machine Learning Community, was die Verwendung von MediaPipe erleichtert.
Es werden folgende Softwareversionen verwendet:
\begin{itemize}
    \item Python 3.12.12 
    \item csv (kommt mit python) 3.12.12 \newline
    Standard-Modul zum Schreiben von Textdateien im "Comma Separated Values" - Format.
    \item math (kommt mit python) 3.12.12 \newline
    Standard-Mathematik-Bibliothek von Python.
    \item cv2 4.12.0.88 (opencv-python) \newline
    Bibliothek für Computer Vision. Ist Notwendig zum lesen der Video Dateien und weiteren Bearbeitung. 
    \item MediaPipe 0.10.21 \newline
    Machine-Learning-Bibliotheken von Google. Wichtig für die Pose Schätzung.
    \item pandas 2.3.3 \newline
    Modul für die Datenanalyse und Tabellenkalkulation. Wichtig für die Umwandlung von CSV in TRC Datei.
    \item numpy 1.26.4 \newline
    Standardbibliothek für mathematische Berechnungen mit Vektoren und Matrizen. Wichtig für die Skalierung.
\end{itemize}
\newpage
\subsection{Videos}
Da das MediaPipe BlazePose Modell in unserem Workflow nur mit Videos arbeitet, brauchen wir als erstes
ein Video von  \textbf{einer} Person. Für eine gute Aufnahme sollte folgendes beachtet werden:
\begin{itemize}
    \item Die Person sollte komplett im Bild sein.
    \item Die Person sollte vom Hintergrund unterscheidbar sein.
    \item Die Beleuchtung sollte ausreichend sein, damit die Kamera die Person gut erkennen kann.
    \item Die Kamera sollte still stehen, damit keine Bewegungsunschärfe entsteht.
\end{itemize}

Unsere erste Funktion gibt uns unsere aufgenommenen Videos mit eingezeichneten Skeletten wieder.
Das ist für einen ersten Überblick interessant, um zu sehen ob MediaPipe Blaze Pose die Position der Keypoints
richtig erkannt hat. Außerdem lässt sich schonmal feststellen wo Probleme bei der Erkennung aufkommen.
Beim Aufruf der Anzeige Funktion kann über einen Parameter eingestellt werden, 
ob die Marker Namen angezeigt werden (Abb. \ref{fig:frame_with_names}) oder nicht (Abb. \ref{fig:frame_no_names}).
Zudem verfügt die Funktion über eine Flag-Variable, 
um das Video bei Bedarf zu drehen, 
falls es nicht in der korrekten Ausrichtung eingespeichert wurde.
\begin{figure}[ht]
    \centering
    % Erstes Unter-Bild
    \begin{subfigure}[bx]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{videoframe.png}
        \caption{Ohne Marker-Namen}
        \label{fig:frame_no_names}
    \end{subfigure}
    \hfill % Fügt flexiblen Platz zwischen den Bildern ein
    % Zweites Unter-Bild
    \begin{subfigure}[bx]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{videoframe_names.png}
        \caption{Mit Marker-Namen}
        \label{fig:frame_with_names}
    \end{subfigure}
    
    % Gemeinsame Haupt-Beschriftung
    \caption{Vergleich der Video-Ausgabe aus dem MediaPipe Workflow}
    \label{fig:video_comparison}
\end{figure}
\newpage
\subsection{MediaPipe Modell}
Das verwendete Pose-Modell wird zu Beginn des Workflows in der Funktion
\texttt{create\_pose} instanziiert. Diese Funktion ermöglicht es, zu jedem Start einer
Pose-Schätzung ein neues Modell zu erzeugen, das noch nicht auf zuvor verarbeiteten
Daten basiert. Dadurch wird sichergestellt, dass beim Wechsel von der statischen zur
dynamischen Aufnahme kein zeitlicher Zusammenhang zwischen den beiden Sequenzen
besteht. Insbesondere wird verhindert, dass die Marker ausgehend von der Endposition
der statischen Aufnahme zur Startposition der dynamischen Bewegung interpoliert oder
„wandern“, wodurch konsistente und unabhängige Initialbedingungen für beide Aufnahmen
gewährleistet werden.

In der folgenden Übersicht sind die für die Modellierung verwendeten Parameter aufgeführt.
Diese Parameter bestimmen maßgeblich das Verhalten und die Genauigkeit des Modells.
Im Folgenden werden daher die einzelnen Parameter mit den für die Ergebnisse dieser Arbeit
verwendeten Werte dargestellt.
\begin{itemize}
    \item \texttt{\textbf{static\_image\_mode} (False)}: Gibt an, dass das Modell für Videosequenzen verwendet wird.
    Dadurch werden erkannte Posen über mehrere Frames hinweg verfolgt, was die Stabilität erhöht
    und die Rechenlast reduziert.
    \item \texttt{\textbf{model\_complexity} (2)}: Bestimmt die Komplexität des verwendeten Pose-Modells.
    Höhere Werte liefern in der Regel genauere Ergebnisse, benötigen jedoch mehr Rechenleistung.
    \item \texttt{\textbf{enable\_segmentation} (False)}: Deaktiviert die Personensegmentierung im Bild. Da nur die
    Körperlandmarks benötigt werden, wird auf diese zusätzliche Berechnung verzichtet.
    \item \texttt{\textbf{min\_detection\_confidence} (0.5)}: Legt die minimale Konfidenz fest, ab der eine Pose
    als erfolgreich erkannt gilt.
    \item \texttt{\textbf{min\_tracking\_confidence} (0.5)}: Legt die minimale Konfidenz für das Tracking der Pose
    zwischen aufeinanderfolgenden Frames fest.
    \item \texttt{\textbf{smooth\_landmarks} (True)}: Aktiviert eine zeitliche Glättung der Landmark-Positionen über 
    mehrere Frames.
\end{itemize}
\newpage
\subsection{CSV aus MediaPipe Daten}
Die Funktion \texttt{pose\_estimation} ruft das MediaPipe BlazePose Modell auf dem Video auf
und extrahiert die 3D Positionen der definierten Keypoints.
Die meisten Bildverarbeitungs-Bibliotheken, unter anderem auch MediaPipe BlazePose, haben den Ursprung oben links im Bild.
Also wird die y-Achse einmal invertiert, sodass das Modell nachher nicht auf dem Kopf steht.

Eine Fehlerquelle ist hier, die Missachtung der Orientierung der Person zur Kamera im Vergleich
zum OpenSim Modell. Dadurch kann es passieren, dass die Daten nach import in OpenSim nicht mit dem Modell
übereinstimmen. Um dem entgegenzuwirken, gibt es die Möglichkeit das Video um festgelegte Winkel
um die y-Achse zu rotieren. Rotiert wird dabei gegen den Uhrzeigersinn um die im Parameter \texttt{rotate\_around\_y\_axis}
angegebene Gradzahl (0, 90, 180 oder 270). Dabei müssen die statische Aufnahme und die Bewegungsaufnahme
gegebenfalls unterschiedlich rotiert werden, je nachdem wie die Person in der jeweiligen Aufnahme zur Kamera steht.

Als Ausgabe erhält man dann eine CSV Datei mit Positions Informationen welche für die Marker Datei später wichtig sind.
Die generierte CSV-Datei folgt einer strikten Struktur, die dynamisch anhand der definierten Marker-Liste erstellt wird. 
\begin{itemize}
    \item \textbf{Zeile 1:} Metadaten zur Framerate (FPS).
    \item \textbf{Zeile 2 (Header):} Spaltenbeschriftungen. Für jeden Marker $n$ werden automatisch drei Spalten (\texttt{Marker$n$\_X}, \texttt{Marker$n$\_Y}, \texttt{Marker$n$\_Z}) generiert.
    \item \textbf{Zeile 3 ff.:} Die Positionsdaten pro Frame.
\end{itemize}

\begin{lstlisting}[caption={Allgemeine Struktur der Ausgabedatei}, label={lst:csv_generic}]
#FPS,30.0
Frame,Marker1_X,Marker1_Y,Marker1_Z,Marker2_X,Marker2_Y,Marker2_Z
0,0.12,-0.55,0.80,0.45,-0.30,0.12
1,0.13,-0.54,0.81,0.46,-0.31,0.13
2,0.13,-0.54,0.81,0.46,-0.31,0.13
...
\end{lstlisting}
\newpage
\subsection{Skalierung der MediaPipe Daten}
Da MediaPipe die erkannten 3D-Koordinaten in einem normierten Raum zurückgibt, müssen diese Koordinaten
skaliert werden, um sie in OpenSim verwenden zu können. Dabei geht es hauptsächlich darum, die Längen
von Körperteilen korrekt darzustellen und nicht über die frames variieren zu lassen und den Ursprung der
MediaPipe Daten auf den Ursprung von OpenSim zu verschieben.

Für die Ursprungsverschiebung muss ein Marker als Referenz gewählt und dessen Position in OpenSim als
Referenzkoordinate übergeben werden. Dann wird die durchschnittliche Koordinate dieses Markers über die
statische Aufnahme berechnet und der Differenzvektor zu der OpenSim Referenzkoordinate bestimmt. Dieser
Vektor wird dann auf alle MediaPipe Koordinaten der statischen Aufnahme addiert, um den Ursprung zu verschieben.

Bei der Bewegungsaufnahme wird hingegen ein abweichendes Vorgehen gewählt. Hier wird der
Differenzvektor nicht aus der gemittelten statischen Aufnahme bestimmt, sondern aus der Position des
Referenzmarkers im ersten Frame der Bewegung. Dieser Vektor dient als initiale Ursprungsverschiebung
und wird auf alle nachfolgenden Frames angewendet. Auf diese Weise startet die Bewegung im Ursprung
des OpenSim-Modells, während gleichzeitig eine Translation des Körpers im Raum über die Zeit erhalten
bleibt.

Die Skalierung der Körperteile kann auf verschiedene Arten erfolgen:
\begin{itemize}
    \item \textbf{Skalierung über die durchschnittliche Länge in der statischen Aufnahme:} Hier werden die Körperteile
    einfach auf die durchschnittlichen Längen in der statischen Aufnahme skaliert.
    \item \textbf{Skalierung über eine bekannte Referenzlänge:} Hierbei wird eine bekannte Länge 
    (z.B. die Länge eines Armes) verwendet. Dann wird ein Faktor zwischen dieser und der durchschnittlichen Länge des verwendeten
    Körperteils in der statischen Aufnahme berechnet und anschließend auf alle durchschnittlichen Längen aller Körperteile angewendet,
    um die skalierten Längen zu erhalten.

    Dies ist besonders nützlich, wenn die Person in der Aufnahme eine erhöhte Distanz zur Kamera hat.
\end{itemize}

Um die eigentliche Skalierung durchführen zu können, müssen zuerst die zu skalierenden Körperteile definiert werden.
Für eine saubere Skalierung und um Winkel zwischen den Körperteilen beizubehalten, können die Körperteile in Gruppen
zusammengefasst werden. Jede Gruppe kann aus mehreren verbundenen Körperteilen bestehen, die zusammen skaliert werden.
Im Code gibt es dafür das Array \texttt{limb\_groups}, welches die Indizes der
zu skalierenden Körperteile im \texttt{landmark\_map} dictionary als Tupel übergibt.
Bei einem einfachen Arme Modell, bestehend aus 6 Markern (jeweils Schulter, Ellbogen, Handgelenk), könnte dies wie folgt aussehen:
\begin{verbatim}
idx Marker_name
0   r_shoulder
1   r_elbow
2   r_wrist
3   l_shoulder
4   l_elbow
5   l_wrist


limb_groups = [
    [(0, 1), (1, 2)],
    [(3, 4), (4, 5)]
]
\end{verbatim}

Bei der Skalierung wird für jedes Tupel der zweite Marker an den ersten angepasst, daher ist die Reihenfolge hier entscheidend.
Nach der Verschiebung eines Markers wird ein Verschiebungsvektor aufaddiert, welcher vor der Skalierung des nächsten
Körperteils auch auf den zweiten Marker des Tupels angewendet wird, um die Winkel beizubehalten.

Wenn der entsprechende Parameter in der \texttt{scale} Funktion gesetzt ist, werden die
zu skalierenden Längen einmal vor (Abb. \ref{fig:unscaled_plot}) und nach (Abb. \ref{fig:scaled_plot}) der Skalierung in einem Diagramm dargestellt, um die Skalierung zu überprüfen.
\begin{figure}[htbp]
    \centering
    \begin{subfigure}{0.49\textwidth}
        \includegraphics[width=\textwidth]{img/lengths_before_scaling_s.png}
        \caption{Längen vor Skalierung}
        \label{fig:unscaled_plot}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \includegraphics[width=\textwidth]{img/lengths_after_scaling_s.png}
        \caption{Längen nach Skalierung}
        \label{fig:scaled_plot}
    \end{subfigure}

    \begin{subfigure}{0.3\textwidth}
        \centering
        \includegraphics[width=\textwidth]{img/scaling_legend.png}
        \caption{Legende}
        \label{fig:scaled_legend}
    \end{subfigure}

    \caption{Skalierungs Diagramme}
\end{figure}

Sind nach der Skalierung noch Abweichungen in den Längen der Körperteile vorhanden, so liegt wahrscheinlich ein
Fehler in der Definition der \texttt{limb\_groups} vor.
\newpage
\subsection{Marker Datei aus CSV}
Nachdem wir jetzt die skalierten Positionsdaten in der CSV Datei haben, müssen wir diese noch in eine
Marker Datei (.trc) umwandeln, welche OpenSim verwenden kann.
Eine weitere Funktion in unserem Workflow erstellt genau diese Marker Datei aus den Positionsdaten in der CSV Datei. 
Wichtig dabei ist dass unsere drei verwendeten Keypoints aus Mediapipe BlazePose
auch den gleichen Namen haben wie die Marker in dem Modell von OpenSim.

\begin{table}[ht]
    \centering
    \caption{Struktur der .trc Datei (Beispielwerte)}
    \label{tab:trc_structure}
    \scriptsize % Schriftgröße anpassen
    \begin{tabular}{l l l l l l l l}
        \hline
        \textbf{Frame\#} & \textbf{Time} & \multicolumn{3}{c}{\textbf{r\_shoulder}} & \multicolumn{3}{c}{\textbf{r\_ellbow}} \\
         & & X1 & Y1 & Z1 & X2 & Y2 & Z2 \\
        \hline
        1 & 0.0000 & -0.0032 & 0.8362 & 0.1700 & -0.0601 & 0.8085 & 0.1700 \\
        2 & 0.0167 & -0.0016 & 0.8358 & 0.1700 & -0.0595 & 0.8103 & 0.1700 \\
        3 & 0.0333 & -0.0009 & 0.8358 & 0.1700 & -0.0591 & 0.8109 & 0.1700 \\
        ... & & & & & & & \\
        \hline
    \end{tabular}
\end{table}
\subsection{Verwendung von Marker Datei in OpenSim}
Anschließend wird diese Marker Datei dann in OpenSim verwendet. Dabei unterscheiden wir die Marker Dateien
in zwei Kategorien.
\begin{itemize}
    \item Statisch:
    Die Person im Video hat eine Pose eingenommen und diese gehalten.
    \item Dynamisch:
    Die Person im Video hat eine Bewegung ausgeführt.
\end{itemize}
Die Statische Marker Datei ist vorallem für das Scale Tool von OpenSim wichtig.\cite{opensim_scale}
Damit haben wir jedoch wenig Erfolg gehabt, da wir zu wenig Marker hatten um unser Modell korrekt zu skalieren.
Das könnte ein weiterer Ansatzpunkt für zukünftige Arbeiten sein.

Jede Art von Marker Datei kann man in OpenSim darstellen unter \textit{File -> Preview experimental Data} 
um die Marker visualisiert im Raum zu sehen.
Als letzten Schritt verwenden wir die Marker Dateien im Inversen Kinematik Tool \cite{opensim_ik} von OpenSim um eine .mot
Bewegungsdatei zu bekommen welche die Gelenkwinkel des Modells, zu unterschiedlichen Zeitpunkten, enthält.
\newpage
\subsection{Genauigkeit der 3D-Pose-Schätzung von MediaPipe}
Da dies ein monokularer Ansatz ist, sind die 3D Positionen von MediaPipe BlazePose
nicht so genau wie bei einem Mehrkamerasystem. Um die Genauigkeit zu überprüfen, haben wir ein
Beispielvideo\cite{mp_video_comp} (Video von cam1) verwendet. Im Video ist eine nahezu perfekte 2D Bewegung in der sichtbaren x-y-Ebene zu sehen, um die Stabilität der
3D Pose Schätzung zu testen. 

Zunächst haben wir das Video mit MediaPipe analysiert,
um die 3D Positionen der Keypoints zu extrahieren. Anschließend haben wir den Prozess wiederholt,
aber diesmal die Z-Koordinaten aller Keypoints auf Null gesetzt, um eine 2D-Analyse zu simulieren.
Danach haben wir die Ergebnisse in OpenSim importiert um sie zu betrachten.

Im Ergebnisvideo\cite{mp_2d3d_video}, so wie in der Abbildung~\ref{fig:2d-3d-vergleich}, ist deutlich zu erkennen, dass die 3D-Analyse von MediaPipe
zu erheblichen Schwankungen in der Z-Achse führt, obwohl die Bewegung nahezu ausschließlich in der x-y-Ebene auf der Höhe der Schulter stattfindet.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.6\textwidth]{2D-3D_comp.png}
    \caption{Vergleich der 2D-(links) und 3D-(rechts) Analyse in OpenSim}
    \label{fig:2d-3d-vergleich} 
\end{figure}

Dies zeigt, dass die 3D Pose Schätzung von MediaPipe in diesem Fall nicht stabil genug ist, um präzise Bewegungsdaten zu liefern.
Im Gegensatz dazu liefert die 2D-Analyse, bei der die Z-Koordinaten auf Null gesetzt wurden, eine viel stabilere und genauere Darstellung der Bewegung.
Dies unterstreicht die Limitationen der 3D Pose Schätzung bei monokularen Systemen und legt nahe, dass für präzise biomechanische Analysen
Mehrkamerasysteme oder andere fortschrittlichere Methoden erforderlich sind.
